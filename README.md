# Crawler in nodejs

Crawler to get the list of professor from  Shanghai Jiao Tong University,

# How I came with the idea

I was frustated with searching for every professor and there research interest so I thought why whouldn't I make a crawler for me.

## Getting Started

Clone and run npm install.

# Add .env file in root directory and add your google credential like this 

GOOGLE_ID='######'
GOOGLE_SECRET='#######'
GOOGLE_REFRESH_TOKEN='######'
GOOGLE_SPREADSHEET_ID='#######'

### Prerequisites

What things you need to install the software and how to install them

```
Nodejs
```

### Installing

A step by step series of examples that tell you how to get a development env running

Clone project

```
git clone https://github.com/SalahudinMalik/nodejs_crawler.git
```


# Roadmap
Planned features & enhancements are:

* Send auto emails to professors

## Built With

* [Nodejs](http://www.nodejs.com) - Nodejs

## Contributing

Please read [CONTRIBUTING.md](https://gist.github.com/PurpleBooth/b24679402957c63ec426) for details on our code of conduct, and the process for submitting pull requests to us.


## Authors

* **Salahudin Malik** - *Initial work* - [SalahudinMalik](https://github.com/SalahuidnMalik)

See also the list of [contributors](https://github.com/salahudinmalik/project) who participated in this project.

## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details

## Acknowledgments

* Thanks to my friend Abdul Hafeez for crawler idea
